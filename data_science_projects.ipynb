{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44728ee3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\" style=\"font-size:60px;\">\n",
    "Probelehrveranstaltung für die Professur für Angewandte Mathematik mit Schwerpunkt Statistical Learning\n",
    "<br><br>\n",
    "Data Science Projects\n",
    "<br><br>\n",
    "Dr. Fabian Spanhel\n",
    "<div/>\n",
    "    \n",
    "<div align=\"left\" style=\"font-size:16px;\">\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2deb888",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Why does this course exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f031f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Recap Data Science Projekt / Challenge\n",
    "\n",
    "Bachelor Course in the 5th semester\n",
    "\n",
    "- Learn some basics of **writing good code which is suitable for production** and usable by ML engineers.\n",
    "- Learn tools and methods to **conduct reproducible data science experiments and to collaborate in a team**.\n",
    "- Consolidate what you have learned through a **collaborative practice project**.\n",
    "\n",
    "Topics:\n",
    "  - Source code and data version control.\n",
    "  - Setting up and implementing a Python project (Virtual environments, project structure, creating a pip-installable package).\n",
    "  - Good coding habits (Style, type hints, documentation, Git hooks).\n",
    "  - Project management and collaborative coding using Git and GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d31c7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Personal motivation for DSC and also here (I have a mathematical background and took me some to grasp how to build a product, I would have highly appreciate it if this was clear)\n",
    "There is a young difference between an ad hoc analysis or building a data product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c8e4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In the last years: [Link to Still Sexy, 2 Min, evtl. aber auch verbal aufzählen bei Data Science Project Workflow] \n",
    "- From on-premise to cloud.\n",
    "- From POC to production --> More Software Engineering.\n",
    "- No Unicorns anymore but the data science role is getting split into multiple specialized roles -> ML & Data Engineering roles have emerged.\n",
    "- MLOps which should facilitate development and operationalization of AI has become more important -> More Software Engineering.\n",
    "- Before specializing in the job market, I think it is good to get an impression of these specialized roles and learn more software engineering -> Objective of this course -> Oder DSC wollte software engineering Konzepte beibringen, das wird hier fortgeführt und aber auch Einblick in die Rollen\n",
    "\n",
    "See also [\"**Is Data Scientist Still the Sexiest Job of the 21st Century?**\"](https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c57833",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- But it’s likely because the data science role is getting split into multiple different titles. And conveniently cheaper ones. \n",
    "- Salaries have decreased (More people, and also shift to ML, reddit links, also influenced by recession, Downgrading data scientists to DA, DS -> ML, DS -> DE)\n",
    "- Very few junior roles, Demand for senior roles\n",
    "- LLMs… (Do we still need DA or people with DL Knowledge, or people how anbinden APIs from the Great Tech Giants?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b18b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science Project Workflow [5 Min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde79ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "No differently from any cross-functional project that requires teams to work together! \n",
    "Organizations need to determine boundaries between these roles in a way that works for everyone, so there can be clarity about responsibilities. \n",
    "\n",
    "- Important to understand how a project looks like\n",
    "- Data\n",
    "- Cross-functional team (Stakeholder, Data Sciencist, Sofware Enigeers)\n",
    "\n",
    "- Nested cross validation (Students were not sicher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26111024",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cloud: AWS [1 min]\n",
    "- Interacting with cloud services is a demanded skill for Data Scientist and will even become more important.\n",
    "- We will cover the fundamental services of AWS in this course and learn how to\n",
    "    - Store and retrieve data, such as files and backups, using **S3**.\n",
    "    - Leverage scalable cloud-based compute capacity using **EC2**.\n",
    "    - To build, train, and deploy ML models with **Sagemaker**.\n",
    "- Optional: Glue & Athena, Lambda functions...\n",
    "- How to interact with various AWS services using Python and the **boto3** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c599f67",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "- - Fully managed machine learning service named Amazon SageMaker. It allows the data scientist to run it on EC2. Data scientists use this tool to build, train, deploy machine learning models, and scale business operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e3013",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Series [6 Min]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2abed",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Let's just pick this/go into detail here because we have also spoken about this in the first lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd8e49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Multi-step forecasts**\n",
    "\n",
    "- In practice, one often has to provide multi-step forecasts $\\big(\\text{Pred}_t[Y_{t+h}]\\big)_{h=1, \\ldots, H}$.\n",
    "- How can we obtain this sequence of forecasts?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281dc46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall: If $Y_t = aY_{t-1} + U_t$ then $\\text{Pred}_t[Y_{t+h}] = a\\text{Pred}_t[Y_{t+h-1}]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fc0ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if $Y_t = f(Y_{t-1}) + U_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b7831",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can we use $\\text{Pred}_t[Y_{t+h}] = f\\left(\\text{Pred}_t[Y_{t+h-1}]\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f9142",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Multi-step forecasts with features**\n",
    "\n",
    "\n",
    "- What if $Y_t = f(Y_{t-1}, X_{t-1}) + U_t$ and we don't know $(X_{t})_{h=t+1,\\ldots, t+H}$? \n",
    "- If $Y_t = aY_{t-1} + bX_{t-1} + U_t$, we have that\n",
    "\n",
    "    $\n",
    "    \\begin{align}\n",
    "    \\text{Pred}_t[Y_{t+1}] & = a\\text{Pred}_t[Y_{t}] + b\\text{Pred}_t[X_{t}]\\phantom{....}\n",
    "    \\end{align}\n",
    "    $\n",
    "\n",
    "    but because we don't know $\\text{Pred}_t[X_{t+1}]$ we cannot compute\n",
    "\n",
    "    $\n",
    "    \\begin{align}\n",
    "    & \\text{Pred}_t[Y_{t+2}] = a\\text{Pred}_t[Y_{t+1}] + b\\text{Pred}_t[X_{t+1}]\n",
    "    \\end{align}\n",
    "    $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e9309",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Possible solutions:\n",
    "    1. Set up a model for $X_t$, e.g., $X_t = r(X_{t-1}, Y_{t-1}) + V_t$, to get the **indirect forecast**\n",
    "    \n",
    "        $\\text{Pred}_t[Y_{t+h}] = f(\\text{Pred}_t[Y_{t+h-1}], \\text{Pred}_t[X_{t+h-1}])$\n",
    "    2. For each forecast horizon $h$, set up a model $Y_{t+h} = f_h(Y_{t-1}, X_{t-1}) + U_{t,h}$ to get the **direct forecast** \n",
    "    \n",
    "        $\\text{Pred}_t[Y_{t+h}] = f_h(Y_{t-1}, X_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e788fafe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Multi-step forecasts with features: Possible solutions**\n",
    "\n",
    "1. Get the **indirect forecast**\n",
    "\n",
    "    $\\text{Pred}_t[Y_{t+h}] = f(\\text{Pred}_t[Y_{t+h-1}], \\text{Pred}_t[X_{t+h-1}])$\n",
    "2. Get the **direct forecast**\n",
    "\n",
    "    $\\text{Pred}_t[Y_{t+h}] = f_h(Y_{t-1}, X_{t-1})$\n",
    "- Note that 1. increases in the number of features, whereas 2. increases in the number of forecast horizons $h$.\n",
    "- How to tune the corresponding models of 1. and 2.?\n",
    "- How can we handle the data to do direct and indirect forecasts?\n",
    "- Which approach is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d43d0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**These questions will be investigated with a hands-on project in this course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec829c6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Certainly, here's a reformulation:\n",
    "\n",
    "\"In practical scenarios, it's common to make predictions for a variable of interest over multiple future time steps. This means that instead of predicting just the next value, you are forecasting a sequence of future values for the variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9748e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Model tuning** \n",
    "\n",
    "- $K$-fold cross-validation is most commonly used to tune the hyperparameters of a model.\n",
    "- It is typically based on the assumption of iid data.\n",
    "- How to do cross-validation when we have temporal dependence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0dc6d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $K$-fold cross-validation can be used in [special cases](https://www.sciencedirect.com/science/article/abs/pii/S0167947317302384).\n",
    "- In general, cross-validation must consider existing dependence of the data to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning).\n",
    "- Data leakage is a [serious problem in academica and industry](doi:10.1016/j.patter.2023.100804).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f0820",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model tuning with time series cross-validation**\n",
    "1. Model Training: A predictive model is trained on \"in-sample\" / training data of length $T_{is}$.\n",
    "2. Validation: The model is scored on \"out-of-sample\" / validation data of length $T_{oos}$.\n",
    "3. Shifting: The end of the \"in-sample\" data is increased to $T_{is} + T_{oos}$.\n",
    "4. Repeat Steps 1 - 3 until $T_{oos} = 0$.\n",
    "3. Performance Evaluation: Aggregate \"out-of-sample\" validation scores.\n",
    "\n",
    "Questions:\n",
    "- How to specify ($T_{is}$, $T_{oos}$) / the resulting folds?\n",
    "- How to optimize the training length? Use $(w_tY_t)_{t=1,\\ldots T_{is}}$ as training data, where $(w_t)_{t=1}^{T}$ is increasing in $t$ and can be obtained via cross-validation?\n",
    "- Should the aggregation of out-of-sample validation scores be weighted equally, or should the results of validation sets closer to today be weighted more heavily?\n",
    "- How to actually split the data into folds? \n",
    "<!-- To the best of my knowledge there is no package available that covers important practical cases \n",
    "(split w.r.t. date, a set of time series, groups)\n",
    "-->\n",
    "**These questions will be investigated with a hands-on project in this course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330bcd7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - to mimic the real-world forecasting scenario -> want to forecast six weeks -> out-of-sample six weeks\n",
    "- these two questions have an enormous practical relevance in my work\n",
    "\n",
    "Time series CV:\n",
    "The dataset is divided into time periods (e.g., months or years).\n",
    "\n",
    "The model is trained on the historical data up to a certain point in time (training set), and then it is validated or tested on the data in the subsequent time period (validation or test set).\n",
    "\n",
    "This process is repeated iteratively by shifting the training and validation periods forward in time until all data points have been considered.\n",
    "\n",
    "The performance of the model is evaluated at each step, and the results are typically aggregated to assess the model's overall predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394a4eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bild time series cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4a446",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLflow [3 Min]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7e883",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Who has heard of MLflow? How have used it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919c481",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[MLflow](https://mlflow.org/) is an open-source platform for managing the machine learning lifecycle with the following features\n",
    "1. Tracking: MLflow tracks experiments, metrics, parameters, and artifacts for easy comparison and result reproducibility.\n",
    "2. Registry: MLflow's model registry organizes and versions models for collaboration and governance.\n",
    "3. Models: MLflow offers a model management component for packaging models in a standard format and deploying them to various platforms,\n",
    "4. UI and API: MLflow offers a web-based UI and REST API for interactive exploration and programmatic access.\n",
    "\n",
    "Integrated in DataBricks and 15.5k stars on GitHub (October 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4612b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "https://mlflow.org/docs/latest/what-is-mlflow.html\n",
    "\n",
    "- [MLflow](https://mlflow.org/) is an open-source platform for managing the end-to-end machine learning lifecycle.\n",
    "- Developed by DataBrics\n",
    "- to provide a consistent way to manage machine learning projects, from data preparation and experimentation to model deployment and monitoring. \n",
    "MLflow offers several key components and features: (Ausschnitt)\n",
    "Overall, MLflow is designed to help data scientists and machine learning engineers with tasks such as tracking experiments, sharing code, managing models, and deploying them into production. It provides a unified and agnostic approach to managing the machine learning lifecycle, making it easier to transition from experimentation to production deployment.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c9be1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**MLflow Demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176e4b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following code snippet uses `mlflow.autlog` to automatically track the cross-validation of a `RandomForestRegressor` on a diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a465d59",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "mlflow.autolog()\n",
    "db = load_diabetes()\n",
    "\n",
    "def run(): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc415801",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now start the MLflow tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998f7c6c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-10 19:00:29 +0200] [23418] [INFO] Starting gunicorn 21.2.0\n",
      "[2023-10-10 19:00:29 +0200] [23418] [INFO] Listening at: http://127.0.0.1:5000 (23418)\n",
      "[2023-10-10 19:00:29 +0200] [23418] [INFO] Using worker: sync\n",
      "[2023-10-10 19:00:29 +0200] [23419] [INFO] Booting worker with pid: 23419\n",
      "[2023-10-10 19:00:30 +0200] [23420] [INFO] Booting worker with pid: 23420\n",
      "[2023-10-10 19:00:30 +0200] [23421] [INFO] Booting worker with pid: 23421\n",
      "[2023-10-10 19:00:30 +0200] [23422] [INFO] Booting worker with pid: 23422\n",
      "^C\n",
      "[2023-10-10 19:02:45 +0200] [23418] [INFO] Handling signal: int\n",
      "[2023-10-10 19:02:45 +0200] [23422] [INFO] Worker exiting (pid: 23422)\n",
      "[2023-10-10 19:02:45 +0200] [23420] [INFO] Worker exiting (pid: 23420)\n",
      "[2023-10-10 19:02:45 +0200] [23421] [INFO] Worker exiting (pid: 23421)\n",
      "[2023-10-10 19:02:45 +0200] [23419] [INFO] Worker exiting (pid: 23419)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2111da2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "An open the [MLflow user interface](http://127.0.0.1:5000) (won't work if you are not executing this presentation on your local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c8b21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Streamlit [2 Min]\n",
    "- Streamlit turns data scripts into shareable web apps in minutes and exhibits the following features.\n",
    "    - Rapid Development: Streamlit simplifies web app creation with minimal Python code, ideal for non-web developers.\n",
    "    - Interactive Widgets & Real-time Updates: It offers various widgets for easy data and model interaction and enables dynamic data visualization.\n",
    "    - Code reuse: Seamlessly integrates with data science libraries (e.g., Pandas, Matplotlib) for code reuse.\n",
    "- Trusted by over 80% of Fortune 50 companies and integrated in Snowflake. 27.8k stars on GitHub (October 2023).\n",
    "- Not the right tool for complex interfaces and/or nested state.\n",
    "- Examples: [Analytics Dashoboard](https://shamiraty-streamlit-dashboard-descriptive-analytics-home-5ks7sm.streamlit.app/), [MathGPT](https://mathgpt.streamlit.app/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db798e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "-> Who knows it? Who was worked with it?\n",
    "\n",
    "Pandas has 40k stars\n",
    "\n",
    "(we also used streamlit to provide LLMs in our company)\n",
    "\n",
    "All in pure Python. No front‑end experience required.\n",
    "\n",
    "Creating web apps entirely in Python is an enticing idea. Setting up a web app requires both frontend and backend skills such as HTML, CSS, JavaScript (and the numerous frameworks) together with Python or some other server-side language on the backend. It’s a lot of work!\n",
    "\n",
    "If instead everything could be handled entirely in Python, potentially in a single file, the speed of development would increase dramatically. Today, there are several libraries that attempt to deliver this experience. The most popular based on stars on GitHub is Streamlit, with 24.9k stars at the time of writing this.\n",
    "\n",
    "While Streamlit is incredibly succinct, enabling the creation of web apps in impressively few lines of code, it is not without drawbacks. It has a very limited ability to make customized interfaces, and it has a rather odd mechanism where everything is rerun every time a state is changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2664bb3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Streamlit is an open-source Python library that is used for creating web applications for data science and machine learning projects with minimal effort. It is designed to make it easy for data scientists and engineers to turn data scripts into shareable web apps quickly. Streamlit simplifies the process of creating interactive and data-driven web applications by providing a high-level API and handling many of the underlying web development tasks.\n",
    "\n",
    "Streamlit is commonly used by data scientists, analysts, and engineers to create data dashboards, interactive data exploration tools, machine learning model demos, and other web applications that showcase and share their data-related work. It has gained popularity due to its simplicity and rapid development capabilities, making it a valuable tool for data science projects and prototyping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c8429",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning objectives (oder am Ende?) [1 Min]\n",
    "- Hands-on Learning with procjets\n",
    "- Cloud\n",
    "- Time Series\n",
    "- Manage machine learning lifecycle with MLflow\n",
    "- Monitor & retrain (?)\n",
    "- Deploy a App with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19b880",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0167947317302384\n",
    "\n",
    "https://www.youtube.com/watch?v=dWhdWxgt5SU\n",
    "\n",
    " Kapoor, Sayash; Narayanan, Arvind (August 2023). \"Leakage and the reproducibility crisis in machine-learning-based science\". Patterns: 100804. doi:10.1016/j.patter.2023.100804. ISSN 2666-3899.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ceb952",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Obsolete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91198a56",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Vision/Language/Audio vs. Tabular Data [5 Min] --> Vll. weg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1edb4ce",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Causal AI [2 Min] (Bilder von Martin Spindler)\n",
    "-> eher nicht, da Inferenz\n",
    "Aber vll. Beispiel mit Performance Spielfilm (Kontextwissen)\n",
    "oder Effekt Anzahl Runs (Censoring) -> dauert zu lange und schwer zu erklären"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc767e79",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Censoring [2 min] -> vll. weg"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "comment_magics": false
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
