{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de235377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\" style=\"font-size:60px;\">\n",
    "Probelehrveranstaltung für die Professur für Angewandte Mathematik mit Schwerpunkt Statistical Learning\n",
    "<br><br>\n",
    "Data Science Projects\n",
    "<br><br>\n",
    "Dr. Fabian Spanhel\n",
    "<div/>\n",
    "    \n",
    "<div align=\"left\" style=\"font-size:16px;\">\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b963413",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "1 Min]\n",
    "1. Welcome to my second presentation which is about data science projects.\n",
    "2. Data science projects is a course that is targeted at master students who like to work in the data science domain.\n",
    "3. This course is the follow-up course **to** the course Data Science Challenge / Projekt which I lecture at the moment for the bachelor students of Data Science and scientific computing.\n",
    "4. This courses emphasis is on practical skills, problem-solving, and the ability to use data science tools and methods in practical scenarios, such as those encountered in industry work situations.\n",
    "5. For this purpose, this course provides hands-on projects and case studies (to highlight and reinforce the practical aspects of data science.)\n",
    "\n",
    "Basically, this is the course I would like to have before I graduate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b57e55",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Recap Data Science Projekt / Challenge\n",
    "\n",
    "Bachelor Course in the 5th semester\n",
    "\n",
    "- Learn some basics of **writing good code which is suitable for production** and usable by ML engineers.\n",
    "- Learn tools and methods to **conduct reproducible data science experiments and to collaborate in a team**.\n",
    "- Consolidate what you have learned through a **collaborative practice project**.\n",
    "\n",
    "Topics:\n",
    "  - Source code and data version control.\n",
    "  - Setting up and implementing a Python project (Virtual environments, project structure, creating a pip-installable package).\n",
    "  - Good coding habits (Style, type hints, documentation, Git hooks).\n",
    "  - Project management and collaborative coding using Git and GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b2bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Personal motivation for DSC and also here (I have a mathematical background and took me some to grasp how to build a product, I would have highly appreciate it if this was clear)\n",
    "There is a young difference between an ad hoc analysis or building a data product\n",
    "\n",
    "\n",
    "or program is geared toward hands-on, real-world application and implementation of data science concepts and techniques. It implies that the emphasis is on practical skills, problem-solving, and the ability to use data science tools and methods in practical scenarios, such as those encountered in industry or everyday work situations.\n",
    "In essence, a \"Data Science Practical\" course or program is one that aims to equip students or participants with the skills and knowledge needed to effectively apply data science in practical, professional settings. It often includes hands-on exercises, projects, or case studies to reinforce the practical aspects of data science.\n",
    "\n",
    "Aim: Connect the dots and fill some holes\n",
    "Practice focus, give the students a cutting edge\n",
    "Give students an edge in an increasingly competitive marketplace\n",
    "Gutes Feedback von den Studierenden, dass das gebraucht wird, Zitat Student „“ und ich habe selber auch die Erfahrung gemacht, dass die Studierenden nicht „business-ready“ sind (nicht so negativ formulieren, eher mehr business ready)\n",
    "Ich bringen in den Kurs mein Wissen aus vielen Praxisprojekten ein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c743692",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## How data science has changed into the last years\n",
    "- From on-premise to cloud.\n",
    "- From POC to production --> More Software Engineering. MLOps has emerged.\n",
    "    - MLOps which should facilitate development and operationalization of AI has become more important -> More Software Engineering.\n",
    "- No unicorns anymore but the data science role is getting split into multiple specialized roles -> ML & Data Engineering roles have emerged.\n",
    "- In general, sofware engineering skills and MLOps become more important\n",
    "- Before specializing in the job market, I think it is good to get an impression of these specialized roles and learn more software engineering -> Objective of this course -> Oder DSC wollte software engineering Konzepte beibringen, das wird hier fortgeführt und aber auch Einblick in die Rollen\n",
    "- ...\n",
    "\n",
    "See also [\"**Is Data Scientist Still the Sexiest Job of the 21st Century?**\"](https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f72467",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How has data science changed in the last years?**\n",
    "- From on-premise to cloud.\n",
    "\n",
    "- From POC to production.\n",
    "- No unicorns anymore but the data science role is getting split into multiple specialized roles \n",
    "\n",
    "    -> ML & Data Engineering roles have emerged.\n",
    "- In general, sofware engineering skills and MLOps have become more important.\n",
    "- ...\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9af3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "See also [\"**Is Data Scientist Still the Sexiest Job of the 21st Century?**\"](https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century)\n",
    "\n",
    "**The goal of this course is to address the changes that have occurred in recent years and provide students with skills that might gain them a competitive advantage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169fd79",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- But it’s likely because the data science role is getting split into multiple different titles. And conveniently cheaper ones. \n",
    "- Salaries have decreased (More people, and also shift to ML, reddit links, also influenced by recession, Downgrading data scientists to DA, DS -> ML, DS -> DE)\n",
    "- Very few junior roles, Demand for senior roles\n",
    "- LLMs… (Do we still need DA or people with DL Knowledge, or people how anbinden APIs from the Great Tech Giants?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a407b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[2 Min]\n",
    "\n",
    "1. What practical skills should students or graduates have these days?\n",
    "2. Well, let's take a look back at the last seven years that have passed since I've been working in the industry.\n",
    "3. First of all, the tools with which data scientist work have changed a lot. Ten years ago, it was like the wild west and there were few tools and standards available.\n",
    "    - Most importantly, analytics as well as development and management of ML models have largely shifted to the cloud for several reasons.\n",
    "4. Moreover, the phase of POCs everywhere has passed and more models are put into production and need to be managed. As a consequence, software engineering becomes more crucial.\n",
    "5. Previously, a data scienctist was a unicorn, meaning they did all required tasks — from conceptualizing the use case, to \n",
    "interfacing with business and technology stakeholders, to developing the algorithm and deploying it into production. While this may still be true, there is a continued differentiation of responsibilities. Like **machine learning engineers**, **data engineers**, **advanced analytics**, and **data oriented product managers**. \n",
    "\n",
    "\n",
    "\n",
    "- Specialization may be key.\n",
    "- Before specializing in the job market, I think it is good to get an impression of these specialized roles and learn more software engineering -> Objective of this course -> Oder DSC wollte software engineering Konzepte beibringen, das wird hier fortgeführt und aber auch Einblick in die Rollen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f717f8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data Science Project Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a86d3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " <img src=\"./figures/dsp.png\" alt=\"Data Science Projects\" style=\"width: 1300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb18a8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[6 Min]\n",
    "1. In general, a data science project is often a cross-functional project that requires team to work together.\n",
    "    1. On the left hand side we have the business intelligence or analytics people, or people with a math/stats background (blue) and on the right we have people with an information science background or software engineers (green).\n",
    "    1. This color gradient should highlight that boundaries between these roles are not strict and evolving.\n",
    "1. The center of a data science project is data.\n",
    "1. This data is used to kick-off a POC or to do an ad hoc analysis.\n",
    "    1. This is a one shot analysis or model building.\n",
    "1. The outcome of this POC or ad hoc analysis could be a report/app.\n",
    "1. Or it could be deployment and management of a machine learning model.\n",
    "    1. This model could also then end in an app or dashboard.\n",
    "\n",
    "1. So what skills are needed in this data science project workflow?\n",
    "1. Regarding POC/ad hoch analyses\n",
    "    - Communication is very important. But this best learned in the industry. (viele Fehler)\n",
    "    - Dev:\n",
    "        - local / cloud + Metaflow\n",
    "        - \n",
    "    - Sensibilisieren Kausalität, Trade-off Prediction und Causal Inference.\n",
    "1. Depending on the organization a data scientist might involved or not involved in MLOps. Irrespective of that, there are skills that a data scientist can process to support the interaction with ML or software engineers. -> passt nicht, hier eher sagen dass da auch stats skills gebraucht werden\n",
    "    - MLOps\n",
    "        - Model, Feature & Target drift\n",
    "1. Regarding Apps/Dashboard.\n",
    "    - Especially for POCs the ability to provide an application is crucial. It is important that a DS can present his results. Früher war das schwierig, weil man einen FrontEnd Entwickler brauchte. Heute ist das spielend einfach mit Streamlit. man muss nur wissen dass es das gibt.\n",
    "    \n",
    "For this presentation, I pick some skills/tools and give a short introduction so that it's possible to get an impression of what to expect from this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837591e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "No differently from any cross-functional project that requires teams to work together! \n",
    "Organizations need to determine boundaries between these roles in a way that works for everyone, so there can be clarity about responsibilities. \n",
    "\n",
    "- Important to understand how a project looks like\n",
    "- Data\n",
    "- Cross-functional team (Stakeholder, Data Sciencist, Sofware Enigeers)\n",
    "\n",
    "- Nested cross validation (Students were not sicher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5da18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**AWS Cloud services**\n",
    "\n",
    "- Interacting with cloud services is a demanded skill for Data Scientist and will become even more important.\n",
    "\n",
    "- We will cover the fundamental services of AWS in this course and learn how to\n",
    "    - Store and retrieve data, such as files and backups, using **S3**.\n",
    "    - Leverage scalable cloud-based compute capacity using **EC2**.\n",
    "    - To build, train, and deploy ML models with **Sagemaker**.\n",
    "- Optional: Glue & Athena, Lambda functions...\n",
    "- How to interact with various AWS services using Python and the **boto3** package.\n",
    "\n",
    "**We will be using AWS for the hands-on project in this course!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e82a07",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " [1 min]\n",
    "- Fully managed machine learning service named Amazon SageMaker. It allows the data scientist to run it on EC2. Data scientists use this tool to build, train, deploy machine learning models, and scale business operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913477d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Methodology: Multi-step forecasts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1b0b5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[6 Min]\n",
    "- Let's just pick this/go into detail here because we have also spoken about this in the first lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8ba52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, one often has to provide multi-step forecasts $\\big(\\text{Pred}_t[Y_{t+h}]\\big)_{h=1, \\ldots, H}$.\n",
    "- How can we obtain this sequence of forecasts?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b309b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall: If $Y_t = aY_{t-1} + U_t$ then $\\text{Pred}_t[Y_{t+h}] = a\\text{Pred}_t[Y_{t+h-1}]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ea872",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if $Y_t = f(Y_{t-1}) + U_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6127a47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can we use $\\text{Pred}_t[Y_{t+h}] = f\\left(\\text{Pred}_t[Y_{t+h-1}]\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6f626",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Multi-step forecasts with features**\n",
    "\n",
    "\n",
    "- What if $Y_t = f(Y_{t-1}, X_{t-1}) + U_t$ and we don't know $(X_{t})_{h=t+1,\\ldots, t+H}$? \n",
    "- If $Y_t = aY_{t-1} + bX_{t-1} + U_t$, we have that\n",
    "\n",
    "    $\n",
    "    \\begin{align}\n",
    "    \\text{Pred}_t[Y_{t+1}] & = aY_{t} + bX_{t}\\phantom{....}\n",
    "    \\end{align}\n",
    "    $\n",
    "\n",
    "    but because we don't know $\\text{Pred}_t[X_{t+1}]$ we cannot compute\n",
    "\n",
    "    $\n",
    "    \\begin{align}\n",
    "    & \\text{Pred}_t[Y_{t+2}] = a\\text{Pred}_t[Y_{t+1}] + b\\text{Pred}_t[X_{t+1}]\n",
    "    \\end{align}\n",
    "    $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a6ef4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Possible solutions:\n",
    "    1. Set up a model for $X_t$, e.g., $X_t = r(X_{t-1}, Y_{t-1}) + V_t$, to get the **indirect forecast**\n",
    "    \n",
    "        $\\text{Pred}_t[Y_{t+h}] = f(\\text{Pred}_t[Y_{t+h-1}], \\text{Pred}_t[X_{t+h-1}])$\n",
    "    2. For each forecast horizon $h$, set up a model $Y_{t+h} = f_h(Y_{t-1}, X_{t-1}) + U_{t,h}$ to get the **direct forecast** \n",
    "    \n",
    "        $\\text{Pred}_t[Y_{t+h}] = f_h(Y_{t-1}, X_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26330cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Multi-step forecasts with features: Possible solutions**\n",
    "\n",
    "1. Get the **indirect forecast**\n",
    "\n",
    "    $\\text{Pred}_t[Y_{t+h}] = f(\\text{Pred}_t[Y_{t+h-1}], \\text{Pred}_t[X_{t+h-1}])$\n",
    "2. Get the **direct forecast**\n",
    "\n",
    "    $\\text{Pred}_t[Y_{t+h}] = f_h(Y_{t-1}, X_{t-1})$\n",
    "- Note that 1. increases in the number of features, whereas 2. increases in the number of forecast horizons $h$.\n",
    "- How to tune the corresponding models of 1. and 2.?\n",
    "- How can we handle the data to do direct and indirect forecasts?\n",
    "- Which approach is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5f0a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**These questions will be investigated with a hands-on project in this course!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227019e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Methodology: Model tuning for time series** \n",
    "\n",
    "- $K$-fold cross-validation is most commonly used to tune the hyperparameters of a model.\n",
    "- It is typically based on the assumption of iid data.\n",
    "- How to do cross-validation when we have temporal dependence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090f8fa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- $K$-fold cross-validation can be used in [special cases](https://www.sciencedirect.com/science/article/abs/pii/S0167947317302384).\n",
    "- In general, cross-validation must consider existing dependence of the data to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning).\n",
    "- Data leakage is a [serious problem in academica and industry](doi:10.1016/j.patter.2023.100804).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e12712",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Model tuning with time series cross-validation**\n",
    "<div style=\"display: flex; align-items: left;\">\n",
    "  <div style=\"flex: 1; padding: 30px; font-size: 25px;\">\n",
    "    <ol style=\"margin: 200; padding: 0\">\n",
    "      <li>Model training: A predictive model is trained on training data of length $T_{train}$.</li>\n",
    "      <li>Validation: The model is scored on validation data of length $T_{val}$.</li>\n",
    "      <li>Shifting: The end of the new training data is increased to $T_{train} + T_{val}$.</li>\n",
    "      <li>Repeat Steps 1 - 3 until $T_{val} = 0$.</li>\n",
    "      <li>Performance evaluation: Aggregate validation scores.</li>\n",
    "    </ol>\n",
    "  </div>\n",
    "  <div style=\"flex: 2; padding: 0px;\">\n",
    "      <img src=\"./figures/ts_split.png\" alt=\"Time Series Cross Validation\" style=\"width: 700px;\"/>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Questions:\n",
    "<ul style=\"margin: 200; padding: 0; font-size: 25px; margin-top: -20px\">\n",
    "  <li>How to specify ($T_{train}$, $T_{val}$) or the resulting folds?</li>\n",
    "  <li>How to optimize the training length? Use $(w_tY_t)_{t=1,\\ldots T_{train}}$ as training data, where $(w_t)_{t=1}^{T}$ is increasing in $t$ and can be obtained via cross-validation?</li>\n",
    "  <li>Should the aggregation of validation scores be weighted equally, or should the results of validation sets closer to today be weighted more heavily?</li>\n",
    "  <li>How to actually split the data into folds? </li>\n",
    "</ul>\n",
    " \n",
    "<!-- To the best of my knowledge there is no package available that covers important practical cases \n",
    "(split w.r.t. date, a set of time series, groups)\n",
    "-->\n",
    "<span style=\"font-size: 30px; margin-top: -20px\">**These questions will be investigated with a hands-on project in this course!**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767d3fe",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Certainly, here's a reformulation:\n",
    "\n",
    "\"In practical scenarios, it's common to make predictions for a variable of interest over multiple future time steps. This means that instead of predicting just the next value, you are forecasting a sequence of future values for the variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c517e6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - to mimic the real-world forecasting scenario -> want to forecast six weeks -> out-of-sample six weeks\n",
    "- these two questions have an enormous practical relevance in my work\n",
    "\n",
    "Time series CV:\n",
    "The dataset is divided into time periods (e.g., months or years).\n",
    "\n",
    "The model is trained on the historical data up to a certain point in time (training set), and then it is validated or tested on the data in the subsequent time period (validation or test set).\n",
    "\n",
    "This process is repeated iteratively by shifting the training and validation periods forward in time until all data points have been considered.\n",
    "\n",
    "The performance of the model is evaluated at each step, and the results are typically aggregated to assess the model's overall predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd5172",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**MLOps: Managing the ML lifecycle with MLflow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d050efe",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Who has heard of MLflow? How have used it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd229db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[MLflow](https://mlflow.org/) is an open-source platform for managing the machine learning lifecycle with the following features\n",
    "1. Tracking: MLflow tracks experiments, metrics, parameters, and artifacts for easy comparison and result reproducibility.\n",
    "2. Registry: MLflow's model registry organizes and versions models for collaboration and governance.\n",
    "3. Models: MLflow offers a model management component for packaging models in a standard format and deploying them to various platforms,\n",
    "4. UI and API: MLflow offers a web-based UI and REST API for interactive exploration and programmatic access.\n",
    "\n",
    "Integrated in DataBricks and 15.5k stars on GitHub (October 2023)\n",
    "\n",
    "**We will use Mlflow for the project in this course to manage the machine learning lifecycle!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb55063",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[3 Min]\n",
    "\n",
    "https://mlflow.org/docs/latest/what-is-mlflow.html\n",
    "\n",
    "- [MLflow](https://mlflow.org/) is an open-source platform for managing the end-to-end machine learning lifecycle.\n",
    "- Developed by DataBrics\n",
    "- to provide a consistent way to manage machine learning projects, from data preparation and experimentation to model deployment and monitoring. \n",
    "MLflow offers several key components and features: (Ausschnitt)\n",
    "Overall, MLflow is designed to help data scientists and machine learning engineers with tasks such as tracking experiments, sharing code, managing models, and deploying them into production. It provides a unified and agnostic approach to managing the machine learning lifecycle, making it easier to transition from experimentation to production deployment.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9248f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**MLflow Demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d1053",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following code snippet uses `mlflow.autlog` to automatically track the cross-validation of a `RandomForestRegressor` on a diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa991b8c",
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "mlflow.autolog()\n",
    "db = load_diabetes()\n",
    "\n",
    "def run(): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b05624",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now start the MLflow tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14095f63",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-11 18:11:47 +0200] [1918] [INFO] Starting gunicorn 21.2.0\n",
      "[2023-10-11 18:11:47 +0200] [1918] [INFO] Listening at: http://127.0.0.1:5000 (1918)\n",
      "[2023-10-11 18:11:47 +0200] [1918] [INFO] Using worker: sync\n",
      "[2023-10-11 18:11:47 +0200] [1919] [INFO] Booting worker with pid: 1919\n",
      "[2023-10-11 18:11:47 +0200] [1920] [INFO] Booting worker with pid: 1920\n",
      "[2023-10-11 18:11:47 +0200] [1921] [INFO] Booting worker with pid: 1921\n",
      "[2023-10-11 18:11:47 +0200] [1922] [INFO] Booting worker with pid: 1922\n",
      "^C\n",
      "[2023-10-11 18:47:07 +0200] [1918] [INFO] Handling signal: int\n",
      "[2023-10-11 18:47:07 +0200] [1919] [INFO] Worker exiting (pid: 1919)\n",
      "[2023-10-11 18:47:07 +0200] [1922] [INFO] Worker exiting (pid: 1922)\n",
      "[2023-10-11 18:47:07 +0200] [1921] [INFO] Worker exiting (pid: 1921)\n",
      "[2023-10-11 18:47:07 +0200] [1920] [INFO] Worker exiting (pid: 1920)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78492810",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And open the [MLflow user interface](http://127.0.0.1:5000) (won't work if you are not executing this presentation on your local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15bd736",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Building Apps with Streamlit**\n",
    "- Streamlit turns data scripts into shareable web apps in minutes and exhibits the following features.\n",
    "    - Rapid Development: Streamlit simplifies web app creation with minimal Python code, ideal for non-web developers.\n",
    "    - Interactive Widgets & Real-time Updates: It offers various widgets for easy data and model interaction and enables dynamic data visualization.\n",
    "    - Code reuse: Seamlessly integrates with data science libraries (e.g., Pandas, Matplotlib) for code reuse.\n",
    "- Trusted by over 80% of Fortune 50 companies and integrated in Snowflake. 27.8k stars on GitHub (October 2023).\n",
    "<!--- Not the right tool for complex interfaces and/or nested state.-->\n",
    "- Examples: [Analytics Dashoboard](https://shamiraty-streamlit-dashboard-descriptive-analytics-home-5ks7sm.streamlit.app/), [MathGPT](https://mathgpt.streamlit.app/).\n",
    "\n",
    "**We will be using Streamlit to build an App for the machine learning model of our project!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe198c69",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[2 Min]\n",
    "-> Who knows it? Who was worked with it?\n",
    "\n",
    "Pandas has 40k stars\n",
    "\n",
    "(we also used streamlit to provide LLMs in our company)\n",
    "\n",
    "All in pure Python. No front‑end experience required.\n",
    "\n",
    "Creating web apps entirely in Python is an enticing idea. Setting up a web app requires both frontend and backend skills such as HTML, CSS, JavaScript (and the numerous frameworks) together with Python or some other server-side language on the backend. It’s a lot of work!\n",
    "\n",
    "If instead everything could be handled entirely in Python, potentially in a single file, the speed of development would increase dramatically. Today, there are several libraries that attempt to deliver this experience. The most popular based on stars on GitHub is Streamlit, with 24.9k stars at the time of writing this.\n",
    "\n",
    "While Streamlit is incredibly succinct, enabling the creation of web apps in impressively few lines of code, it is not without drawbacks. It has a very limited ability to make customized interfaces, and it has a rather odd mechanism where everything is rerun every time a state is changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9398120",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Streamlit is an open-source Python library that is used for creating web applications for data science and machine learning projects with minimal effort. It is designed to make it easy for data scientists and engineers to turn data scripts into shareable web apps quickly. Streamlit simplifies the process of creating interactive and data-driven web applications by providing a high-level API and handling many of the underlying web development tasks.\n",
    "\n",
    "Streamlit is commonly used by data scientists, analysts, and engineers to create data dashboards, interactive data exploration tools, machine learning model demos, and other web applications that showcase and share their data-related work. It has gained popularity due to its simplicity and rapid development capabilities, making it a valuable tool for data science projects and prototyping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d7f64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Learning objectives** \n",
    "<br><br>\n",
    "\n",
    "\n",
    "Students will experience the entire Data Science workflow, from defining the task to serving the model via an application or dashboard, through projects. Specifically, students will be able to\n",
    "\n",
    "- Use cloud services to train a machine learning model.\n",
    "\n",
    "- Deal with common problems commonly encountered when working with tabular data in practical settings.\n",
    "- Recognize the difference between a prediction and a causal inference task and the resulting implications for model building and evaluation.\n",
    "- Correctly perform model tuning under temporal dependence and how to do multi-step forecasts.\n",
    "- Deploy machine learning models via Streamlit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48234665",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "- It focuses on hands-on learning with projects.\n",
    "- Cloud\n",
    "- Time Series\n",
    "- Students learn the fundamentals of manage the machine learning lifecycle with MLflow and AWS Sagemaker.\n",
    "- Monitor & retrain (?)\n",
    "- Deploy a App with Streamlit\n",
    "- Ein Praxisprojekt dass entweder POC -> App oder sogar POC -> Deploy macht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b397dd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- The data sciene project course deepens and extends\n",
    "- - The data science projects course aims to equip students with the skills and knowledge needed to effectively apply data science problemos commonly encountered in practical, professional settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b568a36",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "Bringe mein Praxiswissen ein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19420fbb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "s is on practical skills, problem-solving, and the ability to use data science tools and methods in practical scenarios, such as those encountered in industry or everyday work situations.\n",
    "In essence, a \"Data Science Practical\" course or program is one that aims to equip students or participants with the skills and knowledge needed to effectively apply data science in practical, professional settings. It often includes hands-on exercises, projects, or case studies to reinforce the practical aspects of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2cb4cb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "This hands-on \n",
    "\n",
    "\n",
    "Das Projektstudium zielt darauf ab, die für den beruflichen Alltag als Data Scientist benötigten instrumentalen, systemischen und kommunikativen Kompetenzen zu fördern.\n",
    "\n",
    "Die Studierenden beherrschen den kompletten Data Science Workflow von der Datensammlung bis zur Modellevaluation. Insbesondere sind die Studierenden in der Lage\n",
    "\n",
    "ihr Wissen auf auf eine typische Aufgabe aus ihrem Beruf anzuwenden,\n",
    "im Projekt relevante Informationen zu sammeln, zu bewerten und wissenschaftlich zu reflektieren,\n",
    "Werkzeuge aus dem Studium einzusetzen, um die Projektziele zu erreichen,\n",
    "kompetent zu kommunizieren,\n",
    "fachbezogen zu argumentieren,\n",
    "sich über Ideen und Lösungen auszutauschen,\n",
    "sich selbst - allein und im Team - zu organisieren und\n",
    "Verantwortung im Team zu übernehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac11a9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "[1 Min]\n",
    "or program is geared toward hands-on, real-world application and implementation of data science concepts and techniques. It implies that the emphasis is on practical skills, problem-solving, and the ability to use data science tools and methods in practical scenarios, such as those encountered in industry or everyday work situations.\n",
    "In essence, a \"Data Science Practical\" course or program is one that aims to equip students or participants with the skills and knowledge needed to effectively apply data science in practical, professional settings. It often includes hands-on exercises, projects, or case studies to reinforce the practical aspects of data science.\n",
    "\n",
    "Aim: Connect the dots and fill some holes\n",
    "Practice focus, give the students a cutting edge\n",
    "Give students an edge in an increasingly competitive marketplace\n",
    "Gutes Feedback von den Studierenden, dass das gebraucht wird, Zitat Student „“ und ich habe selber auch die Erfahrung gemacht, dass die Studierenden nicht „business-ready“ sind (nicht so negativ formulieren, eher mehr business ready)\n",
    "Ich bringen in den Kurs mein Wissen aus vielen Praxisprojekten ein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31bb26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References**\n",
    "\n",
    "Bergmeir C., Hyndman R. J., Koo B. \"A note on the validity of cross-validation for evaluating autoregressive time series prediction\". Computational Statistics & Data Analysis, Volume 120, 2018, Pages 70-83.\n",
    "https://doi.org/10.1016/j.csda.2017.11.003.\n",
    "\n",
    "Guts Y. \"Target Leaking in Machine Learning\". AI Ukraine Conference 2018. https://www.youtube.com/watch?v=dWhdWxgt5SU\n",
    "\n",
    "Kapoor, S., Narayanan, A. \"Leakage and the reproducibility crisis in machine-learning-based science\". Patterns, 100804, August 2023. https://doi:10.1016/j.patter.2023.100804.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db35d4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Obsolete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d50f4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Vision/Language/Audio vs. Tabular Data [5 Min] --> Vll. weg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97068490",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Causal AI [2 Min] (Bilder von Martin Spindler)\n",
    "-> eher nicht, da Inferenz\n",
    "Aber vll. Beispiel mit Performance Spielfilm (Kontextwissen)\n",
    "oder Effekt Anzahl Runs (Censoring) -> dauert zu lange und schwer zu erklären"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248a8e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Censoring [2 min] -> vll. weg"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "comment_magics": false
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
